{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemini 2.5 Flash - Token Usage Investigation\n",
        "\n",
        "Exploring:\n",
        "1. What token usage metadata is available from Gemini API\n",
        "2. What parameters are available in GenerateContentConfig\n",
        "3. Whether there are any reasoning/thinking-related fields in responses\n",
        "4. What actual token counts we're getting in our current setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize Gemini client\n",
        "genai_client = genai.Client(\n",
        "    api_key=os.environ.get('GEMINI_API_KEY'),\n",
        "    http_options={'timeout': 120000}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            "A white noise process will have\n",
            "\n",
            "(i) A zero mean\n",
            "\n",
            "(ii) A constant variance\n",
            "\n",
            "(iii) Autocovariances that are constant\n",
            "\n",
            "(iv) Autocovariances that are zero except at lag zero\n",
            "\n",
            "Options:\n",
            "A) (ii), (iii), and (iv) only\n",
            "B) (i), (ii), (iii), and (iv)\n",
            "C) (i) and (ii) only\n",
            "D) (i), (ii), and (iii) only\n",
            "\n",
            "Correct Answer: G\n"
          ]
        }
      ],
      "source": [
        "# Load a random MMLU-Pro question\n",
        "dataset = load_dataset(\"TIGER-Lab/MMLU-Pro\", split=\"test\")\n",
        "\n",
        "# Use a fixed index for reproducibility\n",
        "question_idx = 7269  # Same as in your recent logs\n",
        "question_data = dataset[question_idx]\n",
        "\n",
        "print(\"Question:\")\n",
        "print(question_data['question'])\n",
        "print(\"\\nOptions:\")\n",
        "for i, option in enumerate(question_data['options'][:4]):  # Take first 4 options\n",
        "    print(f\"{chr(65+i)}) {option}\")\n",
        "print(f\"\\nCorrect Answer: {question_data['answer']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt created (length: 539 chars)\n"
          ]
        }
      ],
      "source": [
        "# Create a test prompt (similar to your QA prompts)\n",
        "options = question_data['options'][:4]\n",
        "options_text = \"\\n\".join([f\"Option {chr(65+i)}: {option}\" for i, option in enumerate(options)])\n",
        "\n",
        "prompt = f\"\"\"Answer the following question. You must choose between the options provided.\n",
        "\n",
        "Question: {question_data['question']}\n",
        "\n",
        "{options_text}\n",
        "\n",
        "Provide your answer in the following format:\n",
        "Answer: [A, B, C, or D]\n",
        "Confidence: [percentage between 25-100]%\n",
        "Reasoning: [brief explanation]\"\"\"\n",
        "\n",
        "print(\"Prompt created (length: {} chars)\".format(len(prompt)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: No Reasoning (thinking_budget=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running with thinking_budget=0 (no reasoning)...\n",
            "\n",
            "Response Text:\n",
            "Answer: B\n",
            "Confidence: 100%\n",
            "Reasoning: A white noise process is defined by four key properties:\n",
            "(i) A zero mean: The expected value of the process at any time point is zero.\n",
            "(ii) A constant variance: The variance of the process is the same at all time points.\n",
            "(iii) Autocovariances that are zero except at lag zero: This is the defining characteristic of white noise. It means there is no linear correlation between observations at different time points.\n",
            "(iv) Autocovariances that are constant: This is true because the autocovariance at lag zero is a constant (the variance), and the autocovariances at all other lags are also constant (zero). Therefore, all autocovariances are constant.\n",
            "\n",
            "All four statements are true for a white noise process.\n",
            "\n",
            "======================================================================\n",
            "Usage Metadata:\n",
            "  prompt_token_count: 168\n",
            "  candidates_token_count: 177\n",
            "  total_token_count: 345\n",
            "\n",
            "Full usage_metadata object:\n",
            "cache_tokens_details=None cached_content_token_count=None candidates_token_count=177 candidates_tokens_details=None prompt_token_count=168 prompt_tokens_details=[ModalityTokenCount(\n",
            "  modality=<MediaModality.TEXT: 'TEXT'>,\n",
            "  token_count=168\n",
            ")] thoughts_token_count=None tool_use_prompt_token_count=None tool_use_prompt_tokens_details=None total_token_count=345 traffic_type=None\n"
          ]
        }
      ],
      "source": [
        "print(\"Running with thinking_budget=0 (no reasoning)...\\n\")\n",
        "\n",
        "response_no_thinking = genai_client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.0,\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Response Text:\")\n",
        "print(response_no_thinking.text)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Usage Metadata:\")\n",
        "print(f\"  prompt_token_count: {response_no_thinking.usage_metadata.prompt_token_count}\")\n",
        "print(f\"  candidates_token_count: {response_no_thinking.usage_metadata.candidates_token_count}\")\n",
        "print(f\"  total_token_count: {response_no_thinking.usage_metadata.total_token_count}\")\n",
        "print(\"\\nFull usage_metadata object:\")\n",
        "print(response_no_thinking.usage_metadata)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Default (no thinking_budget specified)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running with DEFAULT settings (no thinking_budget specified)...\n",
            "\n",
            "Response Text:\n",
            "A white noise process, denoted as $W_t$, is defined by the following properties:\n",
            "\n",
            "(i) **A zero mean**: $E[W_t] = 0$ for all $t$. This is a fundamental property of white noise. So, (i) is true.\n",
            "\n",
            "(ii) **A constant variance**: $Var(W_t) = E[W_t^2] - (E[W_t])^2 = E[W_t^2] = \\sigma^2$ for some positive constant $\\sigma^2$. This is also a fundamental property. So, (ii) is true.\n",
            "\n",
            "(iii) **Autocovariances that are constant**: The autocovariance function is defined as $\\gamma_k = Cov(W_t, W_{t-k})$. For a process to be weakly stationary (which white noise is), its autocovariance function must depend only on the lag $k$, and not on the specific time $t$. This means $Cov(W_t, W_{t-k})$ is constant with respect to $t$. In the context of time series, \"constant autocovariances\" often refers to this time-invariance property. If interpreted this way, then (iii) is true. (If it meant $\\gamma_k$ is the same value for all $k$, it would be false, as $\\gamma_0 = \\sigma^2$ and $\\gamma_k = 0$ for $k \\neq 0$). Given the options, this interpretation is most likely intended.\n",
            "\n",
            "(iv) **Autocovariances that are zero except at lag zero**: For white noise, the autocovariance function is specifically:\n",
            "$\\gamma_k = \\begin{cases} \\sigma^2 & \\text{if } k=0 \\\\ 0 & \\text{if } k \\neq 0 \\end{cases}$\n",
            "This means the autocovariance is non-zero only at lag zero (where it equals the variance $\\sigma^2$) and zero for all other lags. So, (iv) is true.\n",
            "\n",
            "Since (i), (ii), (iii) (under the common interpretation of time-invariance for autocovariance in stationary processes), and (iv) are all true properties of a white noise process, the correct option is the one that includes all four statements.\n",
            "\n",
            "The final answer is $\\boxed{B}$\n",
            "Confidence: 100%\n",
            "Reasoning: A white noise process is defined by having a zero mean (i), a constant variance (ii), and autocovariances that are zero for all non-zero lags (iv). Additionally, white noise is a weakly stationary process, which implies its autocovariance function is constant with respect to time (i.e., it depends only on the lag, not on the specific time point $t$). This is the most common interpretation of \"autocovariances that are constant\" in this context (iii). Therefore, all four statements (i), (ii), (iii), and (iv) are correct properties of a white noise process.\n",
            "\n",
            "======================================================================\n",
            "Usage Metadata:\n",
            "  prompt_token_count: 168\n",
            "  candidates_token_count: 653\n",
            "  total_token_count: 4666\n",
            "\n",
            "Full usage_metadata object:\n",
            "cache_tokens_details=None cached_content_token_count=None candidates_token_count=653 candidates_tokens_details=None prompt_token_count=168 prompt_tokens_details=[ModalityTokenCount(\n",
            "  modality=<MediaModality.TEXT: 'TEXT'>,\n",
            "  token_count=168\n",
            ")] thoughts_token_count=3845 tool_use_prompt_token_count=None tool_use_prompt_tokens_details=None total_token_count=4666 traffic_type=None\n"
          ]
        }
      ],
      "source": [
        "print(\"Running with DEFAULT settings (no thinking_budget specified)...\\n\")\n",
        "\n",
        "response_default = genai_client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.0\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Response Text:\")\n",
        "print(response_default.text)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Usage Metadata:\")\n",
        "print(f\"  prompt_token_count: {response_default.usage_metadata.prompt_token_count}\")\n",
        "print(f\"  candidates_token_count: {response_default.usage_metadata.candidates_token_count}\")\n",
        "print(f\"  total_token_count: {response_default.usage_metadata.total_token_count}\")\n",
        "print(\"\\nFull usage_metadata object:\")\n",
        "print(response_default.usage_metadata)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: High Reasoning (thinking_budget=8192)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running with thinking_budget=8192 (high reasoning)...\\n\")\n",
        "\n",
        "response_high_thinking = genai_client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.0,\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=8192)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Response Text:\")\n",
        "print(response_high_thinking.text)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Usage Metadata:\")\n",
        "print(f\"  prompt_token_count: {response_high_thinking.usage_metadata.prompt_token_count}\")\n",
        "print(f\"  candidates_token_count: {response_high_thinking.usage_metadata.candidates_token_count}\")\n",
        "print(f\"  total_token_count: {response_high_thinking.usage_metadata.total_token_count}\")\n",
        "print(\"\\nFull usage_metadata object:\")\n",
        "print(response_high_thinking.usage_metadata)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "comparison = pd.DataFrame([\n",
        "    {\n",
        "        'Configuration': 'No Reasoning (budget=0)',\n",
        "        'Prompt Tokens': response_no_thinking.usage_metadata.prompt_token_count,\n",
        "        'Candidates Tokens': response_no_thinking.usage_metadata.candidates_token_count,\n",
        "        'Total Tokens': response_no_thinking.usage_metadata.total_token_count,\n",
        "        'Response Length (chars)': len(response_no_thinking.text)\n",
        "    },\n",
        "    {\n",
        "        'Configuration': 'Default (no budget)',\n",
        "        'Prompt Tokens': response_default.usage_metadata.prompt_token_count,\n",
        "        'Candidates Tokens': response_default.usage_metadata.candidates_token_count,\n",
        "        'Total Tokens': response_default.usage_metadata.total_token_count,\n",
        "        'Response Length (chars)': len(response_default.text)\n",
        "    },\n",
        "    {\n",
        "        'Configuration': 'High Reasoning (budget=8192)',\n",
        "        'Prompt Tokens': response_high_thinking.usage_metadata.prompt_token_count,\n",
        "        'Candidates Tokens': response_high_thinking.usage_metadata.candidates_token_count,\n",
        "        'Total Tokens': response_high_thinking.usage_metadata.total_token_count,\n",
        "        'Response Length (chars)': len(response_high_thinking.text)\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOKEN USAGE COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# Calculate deltas\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DELTAS (compared to No Reasoning)\")\n",
        "print(\"=\"*70)\n",
        "baseline = response_no_thinking.usage_metadata.total_token_count\n",
        "print(f\"Default vs No Reasoning: {response_default.usage_metadata.total_token_count - baseline:+d} tokens\")\n",
        "print(f\"High Reasoning vs No Reasoning: {response_high_thinking.usage_metadata.total_token_count - baseline:+d} tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Investigate Response Object Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's inspect the full response object to see if there are any hidden fields\n",
        "print(\"Inspecting response object structure...\\n\")\n",
        "print(\"Available attributes on response:\")\n",
        "print([attr for attr in dir(response_high_thinking) if not attr.startswith('_')])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Available attributes on usage_metadata:\")\n",
        "print([attr for attr in dir(response_high_thinking.usage_metadata) if not attr.startswith('_')])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if there's any difference in candidates structure\n",
        "print(\"Candidates structure (high reasoning):\")\n",
        "print(f\"Number of candidates: {len(response_high_thinking.candidates)}\")\n",
        "if response_high_thinking.candidates:\n",
        "    print(f\"\\nFirst candidate attributes:\")\n",
        "    print([attr for attr in dir(response_high_thinking.candidates[0]) if not attr.startswith('_')])\n",
        "    print(f\"\\nFirst candidate content:\")\n",
        "    print(response_high_thinking.candidates[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "Based on the results above:\n",
        "\n",
        "1. **Does default use reasoning tokens?**\n",
        "   - Look at whether Default total_token_count ≈ No Reasoning total_token_count\n",
        "   - If they're similar → Default does NOT use reasoning\n",
        "   - If Default > No Reasoning → Default DOES use reasoning\n",
        "\n",
        "2. **Are reasoning tokens included in total_token_count?**\n",
        "   - If High Reasoning > No Reasoning by a significant amount → YES, included\n",
        "   - The delta tells us approximately how many reasoning tokens were used\n",
        "\n",
        "3. **Can we see reasoning token breakdown?**\n",
        "   - Check if usage_metadata has any additional fields\n",
        "   - Check if candidates contain any reasoning-related fields\n",
        "   - If not found → reasoning tokens are NOT separately reported\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm_judge_debate",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
