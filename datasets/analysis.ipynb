{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented MATH Analysis\n",
    "\n",
    "Analyze agreement between Gemini 3 Flash, Grok 4.1 Fast, and Llama 3.1 8B on generated math questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluations - update path as needed\n",
    "eval_files = sorted(Path(\"data\").glob(\"evaluations_*.json\"))\n",
    "if not eval_files:\n",
    "    raise FileNotFoundError(\"No evaluation files found in data/\")\n",
    "\n",
    "eval_path = eval_files[-1]  # Most recent\n",
    "print(f\"Loading: {eval_path}\")\n",
    "\n",
    "with open(eval_path) as f:\n",
    "    evaluations = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(evaluations)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe\n",
    "rows = []\n",
    "for item in evaluations:\n",
    "    rows.append({\n",
    "        \"idx\": item[\"idx\"],\n",
    "        \"level\": item[\"level\"],\n",
    "        \"subject\": item[\"subject\"],\n",
    "        \"gemini\": item[\"gemini_answer_idx\"],\n",
    "        \"grok\": item[\"evaluations\"].get(\"grok\", {}).get(\"answer\"),\n",
    "        \"llama\": item[\"evaluations\"].get(\"llama\", {}).get(\"answer\")\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to ground truth: cases where Gemini and Grok agree\n",
    "df[\"gemini_grok_agree\"] = df[\"gemini\"] == df[\"grok\"]\n",
    "df_gt = df[df[\"gemini_grok_agree\"]].copy()\n",
    "\n",
    "print(f\"Ground truth questions (Gemini == Grok): {len(df_gt)}/{len(df)} ({100*len(df_gt)/len(df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama accuracy against ground truth\n",
    "df_gt[\"llama_correct\"] = df_gt[\"llama\"] == df_gt[\"gemini\"]\n",
    "\n",
    "overall_acc = df_gt[\"llama_correct\"].mean()\n",
    "print(f\"Llama 3.1 8B overall accuracy (vs GT): {100*overall_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance table by level x subject\n",
    "SUBJECTS = [\n",
    "    \"algebra\", \"counting_and_probability\", \"geometry\",\n",
    "    \"intermediate_algebra\", \"number_theory\", \"prealgebra\", \"precalculus\"\n",
    "]\n",
    "\n",
    "def calc_accuracy(group):\n",
    "    if len(group) == 0:\n",
    "        return None\n",
    "    return group[\"llama_correct\"].mean()\n",
    "\n",
    "pivot = df_gt.groupby([\"level\", \"subject\"]).apply(calc_accuracy).unstack(level=\"subject\")\n",
    "pivot = pivot.reindex(columns=SUBJECTS)\n",
    "\n",
    "# Add row/column totals\n",
    "pivot[\"TOTAL\"] = df_gt.groupby(\"level\").apply(calc_accuracy)\n",
    "subject_totals = df_gt.groupby(\"subject\").apply(calc_accuracy)\n",
    "pivot.loc[\"TOTAL\"] = subject_totals.reindex(SUBJECTS).tolist() + [overall_acc]\n",
    "\n",
    "# Format as percentages\n",
    "pivot_pct = (pivot * 100).round(1)\n",
    "pivot_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample counts per cell\n",
    "counts = df_gt.groupby([\"level\", \"subject\"]).size().unstack(level=\"subject\", fill_value=0)\n",
    "counts = counts.reindex(columns=SUBJECTS, fill_value=0)\n",
    "counts[\"TOTAL\"] = counts.sum(axis=1)\n",
    "counts.loc[\"TOTAL\"] = counts.sum(axis=0)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "print(\"Model Agreement Summary\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Total questions: {len(df)}\")\n",
    "print(f\"Gemini-Grok agree (GT): {len(df_gt)} ({100*len(df_gt)/len(df):.1f}%)\")\n",
    "print(f\"Llama accuracy vs GT: {100*overall_acc:.1f}%\")\n",
    "print()\n",
    "print(\"Llama accuracy by level:\")\n",
    "for level in range(1, 6):\n",
    "    subset = df_gt[df_gt[\"level\"] == level]\n",
    "    if len(subset) > 0:\n",
    "        acc = subset[\"llama_correct\"].mean()\n",
    "        print(f\"  Level {level}: {100*acc:.1f}% (n={len(subset)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
