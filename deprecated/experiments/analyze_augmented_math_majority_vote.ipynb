{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FILE = Path(\"majority_vote_augmented_math_generated_20260121_122552_20260121_130655.json\")\n",
    "\n",
    "with open(RESULTS_FILE) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "config = data['config']\n",
    "results = data['results']\n",
    "gt_summary = data['ground_truth_summary']\n",
    "\n",
    "print(f\"Loaded {len(results)} problems\")\n",
    "print(f\"LLaMA runs per problem: {config['llama_runs']}\")\n",
    "print(f\"Oracle: Flash\")\n",
    "print(f\"\\nGround Truth Summary:\")\n",
    "print(f\"  Valid: {gt_summary['valid_count']}\")\n",
    "print(f\"  None:  {gt_summary['none_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(answer):\n",
    "    if answer is None:\n",
    "        return None\n",
    "    answer = str(answer).strip().lower()\n",
    "    try:\n",
    "        num = float(answer)\n",
    "        if num == int(num):\n",
    "            return str(int(num))\n",
    "        return f\"{num:.10f}\".rstrip('0').rstrip('.')\n",
    "    except ValueError:\n",
    "        return answer\n",
    "\n",
    "def answers_match(a, b):\n",
    "    if a is None or b is None:\n",
    "        return False\n",
    "    norm_a = normalize_answer(a)\n",
    "    norm_b = normalize_answer(b)\n",
    "    if norm_a == norm_b:\n",
    "        return True\n",
    "    try:\n",
    "        num_a = float(norm_a) if norm_a else None\n",
    "        num_b = float(norm_b) if norm_b else None\n",
    "        if num_a is not None and num_b is not None:\n",
    "            if abs(num_b) > 1:\n",
    "                return abs(num_a - num_b) / abs(num_b) < 1e-4\n",
    "            return abs(num_a - num_b) < 1e-6\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview by Level and Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_data = []\n",
    "for idx, r in results.items():\n",
    "    overview_data.append({\n",
    "        'idx': int(idx),\n",
    "        'subject': r['subject'],\n",
    "        'level': r['level'],\n",
    "        'ground_truth': r['ground_truth'],\n",
    "        'gt_is_none': r['ground_truth'] is None\n",
    "    })\n",
    "\n",
    "df_overview = pd.DataFrame(overview_data)\n",
    "\n",
    "print(\"Problems by Subject and Level\")\n",
    "print(\"=\" * 60)\n",
    "pivot = df_overview.pivot_table(index='subject', columns='level', aggfunc='size', fill_value=0)\n",
    "pivot['Total'] = pivot.sum(axis=1)\n",
    "pivot.loc['Total'] = pivot.sum()\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFlash Oracle Null Counts by Subject\")\n",
    "print(\"=\" * 50)\n",
    "null_by_subject = df_overview.groupby('subject')['gt_is_none'].agg(['sum', 'count'])\n",
    "null_by_subject.columns = ['nulls', 'total']\n",
    "null_by_subject['pct'] = (null_by_subject['nulls'] / null_by_subject['total'] * 100).round(1)\n",
    "print(null_by_subject)\n",
    "print(f\"\\nTotal nulls: {df_overview['gt_is_none'].sum()}/{len(df_overview)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA Pass@k Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_data = []\n",
    "for idx, r in results.items():\n",
    "    gt = r['ground_truth']\n",
    "    if gt is None:\n",
    "        continue\n",
    "    \n",
    "    llama_answers = [lr['parsed_answer'] for lr in r['llama_results']]\n",
    "    correct_flags = [answers_match(ans, gt) for ans in llama_answers]\n",
    "    \n",
    "    llama_data.append({\n",
    "        'idx': int(idx),\n",
    "        'subject': r['subject'],\n",
    "        'level': r['level'],\n",
    "        'ground_truth': gt,\n",
    "        'llama_answers': llama_answers,\n",
    "        'correct_flags': correct_flags,\n",
    "        'num_correct': sum(correct_flags),\n",
    "        'num_runs': len(correct_flags)\n",
    "    })\n",
    "\n",
    "df_llama = pd.DataFrame(llama_data)\n",
    "print(f\"Problems with valid GT: {len(df_llama)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_at_k_first(correct_flags_list, k):\n",
    "    \"\"\"Pass@k (first k): is any of the first k samples correct?\"\"\"\n",
    "    passes = 0\n",
    "    for correct_flags in correct_flags_list:\n",
    "        if any(correct_flags[:k]):\n",
    "            passes += 1\n",
    "    return passes / len(correct_flags_list) if correct_flags_list else 0\n",
    "\n",
    "def pass_at_k_unbiased(correct_flags_list, k):\n",
    "    \"\"\"Pass@k (unbiased): expected probability of at least one correct in k random samples.\"\"\"\n",
    "    from math import comb\n",
    "    total = 0\n",
    "    for correct_flags in correct_flags_list:\n",
    "        n = len(correct_flags)\n",
    "        c = sum(correct_flags)\n",
    "        if c == 0:\n",
    "            prob = 0.0\n",
    "        elif n - c < k:\n",
    "            prob = 1.0\n",
    "        else:\n",
    "            prob = 1.0 - comb(n - c, k) / comb(n, k)\n",
    "        total += prob\n",
    "    return total / len(correct_flags_list) if correct_flags_list else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 2, 4]\n",
    "\n",
    "def compute_pass_at_k(df, pass_fn, group_col):\n",
    "    results = []\n",
    "    row = {group_col: 'OVERALL', 'n': len(df)}\n",
    "    for k in k_values:\n",
    "        row[f'pass@{k}'] = pass_fn(df['correct_flags'].tolist(), k)\n",
    "    results.append(row)\n",
    "    \n",
    "    for val in sorted(df[group_col].unique()):\n",
    "        subset = df[df[group_col] == val]\n",
    "        row = {group_col: val, 'n': len(subset)}\n",
    "        for k in k_values:\n",
    "            row[f'pass@{k}'] = pass_fn(subset['correct_flags'].tolist(), k)\n",
    "        results.append(row)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def print_pass_at_k_table(df, title, group_col):\n",
    "    print(title)\n",
    "    print(\"=\" * 70)\n",
    "    header = f\"{group_col:<25} {'n':>5}\"\n",
    "    for k in k_values:\n",
    "        header += f\" {'pass@'+str(k):>10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in df.iterrows():\n",
    "        line = f\"{str(row[group_col]):<25} {row['n']:>5}\"\n",
    "        for k in k_values:\n",
    "            line += f\" {row[f'pass@{k}']:>10.2%}\"\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pass@k by Subject (Unbiased Estimator)\")\n",
    "print()\n",
    "df_by_subject = compute_pass_at_k(df_llama, pass_at_k_unbiased, 'subject')\n",
    "print_pass_at_k_table(df_by_subject, \"\", 'subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pass@k by Level (Unbiased Estimator)\")\n",
    "print()\n",
    "df_by_level = compute_pass_at_k(df_llama, pass_at_k_unbiased, 'level')\n",
    "print_pass_at_k_table(df_by_level, \"\", 'level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pass@k Heatmap: Subject x Level (pass@1 unbiased)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "heatmap_data = []\n",
    "for subject in sorted(df_llama['subject'].unique()):\n",
    "    row = {'subject': subject}\n",
    "    for level in sorted(df_llama['level'].unique()):\n",
    "        subset = df_llama[(df_llama['subject'] == subject) & (df_llama['level'] == level)]\n",
    "        if len(subset) > 0:\n",
    "            row[f'L{level}'] = pass_at_k_unbiased(subset['correct_flags'].tolist(), 1)\n",
    "        else:\n",
    "            row[f'L{level}'] = None\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "df_heatmap = pd.DataFrame(heatmap_data).set_index('subject')\n",
    "df_heatmap_pct = df_heatmap.applymap(lambda x: f\"{x:.0%}\" if x is not None else \"-\")\n",
    "print(df_heatmap_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of Correct Answers per Problem\")\n",
    "print(\"=\" * 50)\n",
    "correct_dist = df_llama['num_correct'].value_counts().sort_index()\n",
    "for num_correct, count in correct_dist.items():\n",
    "    pct = count / len(df_llama) * 100\n",
    "    print(f\"  {num_correct}/4 correct: {count:3d} problems ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProblems where LLaMA got 0/4 correct (hardest)\")\n",
    "print(\"=\" * 80)\n",
    "zero_correct = df_llama[df_llama['num_correct'] == 0]\n",
    "print(f\"Total: {len(zero_correct)} problems\\n\")\n",
    "\n",
    "print(\"By Subject:\")\n",
    "for subject, count in zero_correct['subject'].value_counts().items():\n",
    "    print(f\"  {subject}: {count}\")\n",
    "\n",
    "print(\"\\nBy Level:\")\n",
    "for level, count in zero_correct['level'].value_counts().sort_index().items():\n",
    "    print(f\"  Level {level}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProblems where LLaMA got 4/4 correct (easiest)\")\n",
    "print(\"=\" * 80)\n",
    "all_correct = df_llama[df_llama['num_correct'] == 4]\n",
    "print(f\"Total: {len(all_correct)} problems\\n\")\n",
    "\n",
    "print(\"By Subject:\")\n",
    "for subject, count in all_correct['subject'].value_counts().items():\n",
    "    print(f\"  {subject}: {count}\")\n",
    "\n",
    "print(\"\\nBy Level:\")\n",
    "for level, count in all_correct['level'].value_counts().sort_index().items():\n",
    "    print(f\"  Level {level}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample of 0/4 correct problems\")\n",
    "print(\"=\" * 80)\n",
    "for _, row in zero_correct.head(10).iterrows():\n",
    "    print(f\"\\nidx={row['idx']} [{row['subject']}] Level {row['level']}\")\n",
    "    print(f\"GT: {str(row['ground_truth'])[:60]}\")\n",
    "    answers = Counter(row['llama_answers'])\n",
    "    top = answers.most_common(3)\n",
    "    print(f\"LLaMA answers: {', '.join([f'{str(a)[:30]}({c})' for a, c in top])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Majority Vote Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_majority_answer(answers):\n",
    "    counts = Counter(answers)\n",
    "    if not counts:\n",
    "        return None, 0\n",
    "    most_common = counts.most_common(1)[0]\n",
    "    return most_common[0], most_common[1]\n",
    "\n",
    "majority_correct = 0\n",
    "majority_total = len(df_llama)\n",
    "\n",
    "for _, row in df_llama.iterrows():\n",
    "    majority_ans, count = get_majority_answer(row['llama_answers'])\n",
    "    if answers_match(majority_ans, row['ground_truth']):\n",
    "        majority_correct += 1\n",
    "\n",
    "print(f\"Majority vote accuracy: {majority_correct}/{majority_total} ({majority_correct/majority_total:.1%})\")\n",
    "print(f\"Pass@1 (single sample):  {pass_at_k_unbiased(df_llama['correct_flags'].tolist(), 1):.1%}\")\n",
    "print(f\"Pass@4 (any of 4):       {pass_at_k_unbiased(df_llama['correct_flags'].tolist(), 4):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Majority Vote Accuracy by Subject\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for subject in sorted(df_llama['subject'].unique()):\n",
    "    subset = df_llama[df_llama['subject'] == subject]\n",
    "    correct = 0\n",
    "    for _, row in subset.iterrows():\n",
    "        majority_ans, _ = get_majority_answer(row['llama_answers'])\n",
    "        if answers_match(majority_ans, row['ground_truth']):\n",
    "            correct += 1\n",
    "    print(f\"{subject:<25}: {correct:3d}/{len(subset):3d} ({correct/len(subset):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Majority Vote Accuracy by Level\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for level in sorted(df_llama['level'].unique()):\n",
    "    subset = df_llama[df_llama['level'] == level]\n",
    "    correct = 0\n",
    "    for _, row in subset.iterrows():\n",
    "        majority_ans, _ = get_majority_answer(row['llama_answers'])\n",
    "        if answers_match(majority_ans, row['ground_truth']):\n",
    "            correct += 1\n",
    "    print(f\"Level {level}: {correct:3d}/{len(subset):3d} ({correct/len(subset):.1%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
