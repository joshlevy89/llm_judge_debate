{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 350 problems\n",
      "LLaMA runs per problem: 4\n",
      "Oracle: Flash\n",
      "\n",
      "Ground Truth Summary:\n",
      "  Valid: 350\n",
      "  None:  0\n"
     ]
    }
   ],
   "source": [
    "RESULTS_FILE = Path(\"majority_vote_augmented_math_generated_20260121_122552_20260121_130655.json\")\n",
    "\n",
    "with open(RESULTS_FILE) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "config = data['config']\n",
    "results = data['results']\n",
    "gt_summary = data['ground_truth_summary']\n",
    "\n",
    "print(f\"Loaded {len(results)} problems\")\n",
    "print(f\"LLaMA runs per problem: {config['llama_runs']}\")\n",
    "print(f\"Oracle: Flash\")\n",
    "print(f\"\\nGround Truth Summary:\")\n",
    "print(f\"  Valid: {gt_summary['valid_count']}\")\n",
    "print(f\"  None:  {gt_summary['none_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(answer):\n",
    "    if answer is None:\n",
    "        return None\n",
    "    answer = str(answer).strip().lower()\n",
    "    try:\n",
    "        num = float(answer)\n",
    "        if num == int(num):\n",
    "            return str(int(num))\n",
    "        return f\"{num:.10f}\".rstrip('0').rstrip('.')\n",
    "    except ValueError:\n",
    "        return answer\n",
    "\n",
    "def answers_match(a, b):\n",
    "    if a is None or b is None:\n",
    "        return False\n",
    "    norm_a = normalize_answer(a)\n",
    "    norm_b = normalize_answer(b)\n",
    "    if norm_a == norm_b:\n",
    "        return True\n",
    "    try:\n",
    "        num_a = float(norm_a) if norm_a else None\n",
    "        num_b = float(norm_b) if norm_b else None\n",
    "        if num_a is not None and num_b is not None:\n",
    "            if abs(num_b) > 1:\n",
    "                return abs(num_a - num_b) / abs(num_b) < 1e-4\n",
    "            return abs(num_a - num_b) < 1e-6\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview by Level and Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems by Subject and Level\n",
      "============================================================\n",
      "level                      1   2   3   4   5  Total\n",
      "subject                                            \n",
      "algebra                   10  10  10  10  10     50\n",
      "counting_and_probability  10  10  10  10  10     50\n",
      "geometry                  10  10  10  10  10     50\n",
      "intermediate_algebra      10  10  10  10  10     50\n",
      "number_theory             10  10  10  10  10     50\n",
      "prealgebra                10  10  10  10  10     50\n",
      "precalculus               10  10  10  10  10     50\n",
      "Total                     70  70  70  70  70    350\n"
     ]
    }
   ],
   "source": [
    "overview_data = []\n",
    "for idx, r in results.items():\n",
    "    overview_data.append({\n",
    "        'idx': int(idx),\n",
    "        'subject': r['subject'],\n",
    "        'level': r['level'],\n",
    "        'ground_truth': r['ground_truth'],\n",
    "        'gt_is_none': r['ground_truth'] is None\n",
    "    })\n",
    "\n",
    "df_overview = pd.DataFrame(overview_data)\n",
    "\n",
    "print(\"Problems by Subject and Level\")\n",
    "print(\"=\" * 60)\n",
    "pivot = df_overview.pivot_table(index='subject', columns='level', aggfunc='size', fill_value=0)\n",
    "pivot['Total'] = pivot.sum(axis=1)\n",
    "pivot.loc['Total'] = pivot.sum()\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flash Oracle Null Counts by Subject\n",
      "==================================================\n",
      "                          nulls  total  pct\n",
      "subject                                    \n",
      "algebra                       0     50  0.0\n",
      "counting_and_probability      0     50  0.0\n",
      "geometry                      0     50  0.0\n",
      "intermediate_algebra          0     50  0.0\n",
      "number_theory                 0     50  0.0\n",
      "prealgebra                    0     50  0.0\n",
      "precalculus                   0     50  0.0\n",
      "\n",
      "Total nulls: 0/350\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFlash Oracle Null Counts by Subject\")\n",
    "print(\"=\" * 50)\n",
    "null_by_subject = df_overview.groupby('subject')['gt_is_none'].agg(['sum', 'count'])\n",
    "null_by_subject.columns = ['nulls', 'total']\n",
    "null_by_subject['pct'] = (null_by_subject['nulls'] / null_by_subject['total'] * 100).round(1)\n",
    "print(null_by_subject)\n",
    "print(f\"\\nTotal nulls: {df_overview['gt_is_none'].sum()}/{len(df_overview)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA Pass@k Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems with valid GT: 350\n"
     ]
    }
   ],
   "source": [
    "llama_data = []\n",
    "for idx, r in results.items():\n",
    "    gt = r['ground_truth']\n",
    "    if gt is None:\n",
    "        continue\n",
    "    \n",
    "    llama_answers = [lr['parsed_answer'] for lr in r['llama_results']]\n",
    "    correct_flags = [answers_match(ans, gt) for ans in llama_answers]\n",
    "    \n",
    "    llama_data.append({\n",
    "        'idx': int(idx),\n",
    "        'subject': r['subject'],\n",
    "        'level': r['level'],\n",
    "        'ground_truth': gt,\n",
    "        'llama_answers': llama_answers,\n",
    "        'correct_flags': correct_flags,\n",
    "        'num_correct': sum(correct_flags),\n",
    "        'num_runs': len(correct_flags)\n",
    "    })\n",
    "\n",
    "df_llama = pd.DataFrame(llama_data)\n",
    "print(f\"Problems with valid GT: {len(df_llama)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_at_k_first(correct_flags_list, k):\n",
    "    \"\"\"Pass@k (first k): is any of the first k samples correct?\"\"\"\n",
    "    passes = 0\n",
    "    for correct_flags in correct_flags_list:\n",
    "        if any(correct_flags[:k]):\n",
    "            passes += 1\n",
    "    return passes / len(correct_flags_list) if correct_flags_list else 0\n",
    "\n",
    "def pass_at_k_unbiased(correct_flags_list, k):\n",
    "    \"\"\"Pass@k (unbiased): expected probability of at least one correct in k random samples.\"\"\"\n",
    "    from math import comb\n",
    "    total = 0\n",
    "    for correct_flags in correct_flags_list:\n",
    "        n = len(correct_flags)\n",
    "        c = sum(correct_flags)\n",
    "        if c == 0:\n",
    "            prob = 0.0\n",
    "        elif n - c < k:\n",
    "            prob = 1.0\n",
    "        else:\n",
    "            prob = 1.0 - comb(n - c, k) / comb(n, k)\n",
    "        total += prob\n",
    "    return total / len(correct_flags_list) if correct_flags_list else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 2, 4]\n",
    "\n",
    "def compute_pass_at_k(df, pass_fn, group_col):\n",
    "    results = []\n",
    "    row = {group_col: 'OVERALL', 'n': len(df)}\n",
    "    for k in k_values:\n",
    "        row[f'pass@{k}'] = pass_fn(df['correct_flags'].tolist(), k)\n",
    "    results.append(row)\n",
    "    \n",
    "    for val in sorted(df[group_col].unique()):\n",
    "        subset = df[df[group_col] == val]\n",
    "        row = {group_col: val, 'n': len(subset)}\n",
    "        for k in k_values:\n",
    "            row[f'pass@{k}'] = pass_fn(subset['correct_flags'].tolist(), k)\n",
    "        results.append(row)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def print_pass_at_k_table(df, title, group_col):\n",
    "    print(title)\n",
    "    print(\"=\" * 70)\n",
    "    header = f\"{group_col:<25} {'n':>5}\"\n",
    "    for k in k_values:\n",
    "        header += f\" {'pass@'+str(k):>10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in df.iterrows():\n",
    "        line = f\"{str(row[group_col]):<25} {row['n']:>5}\"\n",
    "        for k in k_values:\n",
    "            line += f\" {row[f'pass@{k}']:>10.2%}\"\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@k by Subject (Unbiased Estimator)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "subject                       n     pass@1     pass@2     pass@4\n",
      "----------------------------------------------------------------------\n",
      "OVERALL                     350     16.14%     23.71%     31.43%\n",
      "algebra                      50     21.50%     28.00%     32.00%\n",
      "counting_and_probability     50     16.50%     24.00%     32.00%\n",
      "geometry                     50      8.00%     13.00%     20.00%\n",
      "intermediate_algebra         50      8.50%     14.33%     20.00%\n",
      "number_theory                50     14.50%     22.33%     34.00%\n",
      "prealgebra                   50     31.50%     44.67%     54.00%\n",
      "precalculus                  50     12.50%     19.67%     28.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Pass@k by Subject (Unbiased Estimator)\")\n",
    "print()\n",
    "df_by_subject = compute_pass_at_k(df_llama, pass_at_k_unbiased, 'subject')\n",
    "print_pass_at_k_table(df_by_subject, \"\", 'subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@k by Level (Unbiased Estimator)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "level                         n     pass@1     pass@2     pass@4\n",
      "----------------------------------------------------------------------\n",
      "OVERALL                     350     16.14%     23.71%     31.43%\n",
      "1                            70     19.64%     27.38%     35.71%\n",
      "2                            70     22.14%     34.05%     44.29%\n",
      "3                            70     16.79%     24.05%     30.00%\n",
      "4                            70     13.57%     20.24%     30.00%\n",
      "5                            70      8.57%     12.86%     17.14%\n"
     ]
    }
   ],
   "source": [
    "print(\"Pass@k by Level (Unbiased Estimator)\")\n",
    "print()\n",
    "df_by_level = compute_pass_at_k(df_llama, pass_at_k_unbiased, 'level')\n",
    "print_pass_at_k_table(df_by_level, \"\", 'level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@k Heatmap: Subject x Level (pass@1 unbiased)\n",
      "======================================================================\n",
      "                           L1   L2   L3   L4   L5\n",
      "subject                                          \n",
      "algebra                   48%  30%  15%  12%   2%\n",
      "counting_and_probability  35%  25%   5%  10%   8%\n",
      "geometry                   8%  12%   8%  12%   0%\n",
      "intermediate_algebra       0%  18%  12%   8%   5%\n",
      "number_theory             28%  10%  12%  12%  10%\n",
      "prealgebra                15%  50%  32%  35%  25%\n",
      "precalculus                5%  10%  32%   5%  10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/xmd1jn1s1gg47vfyv_n8g3xh0000gn/T/ipykernel_79378/821169085.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_heatmap_pct = df_heatmap.applymap(lambda x: f\"{x:.0%}\" if x is not None else \"-\")\n"
     ]
    }
   ],
   "source": [
    "print(\"Pass@k Heatmap: Subject x Level (pass@1 unbiased)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "heatmap_data = []\n",
    "for subject in sorted(df_llama['subject'].unique()):\n",
    "    row = {'subject': subject}\n",
    "    for level in sorted(df_llama['level'].unique()):\n",
    "        subset = df_llama[(df_llama['subject'] == subject) & (df_llama['level'] == level)]\n",
    "        if len(subset) > 0:\n",
    "            row[f'L{level}'] = pass_at_k_unbiased(subset['correct_flags'].tolist(), 1)\n",
    "        else:\n",
    "            row[f'L{level}'] = None\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "df_heatmap = pd.DataFrame(heatmap_data).set_index('subject')\n",
    "df_heatmap_pct = df_heatmap.applymap(lambda x: f\"{x:.0%}\" if x is not None else \"-\")\n",
    "print(df_heatmap_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Correct Answers per Problem\n",
      "==================================================\n",
      "  0/4 correct: 240 problems ( 68.6%)\n",
      "  1/4 correct:  45 problems ( 12.9%)\n",
      "  2/4 correct:  27 problems (  7.7%)\n",
      "  3/4 correct:  25 problems (  7.1%)\n",
      "  4/4 correct:  13 problems (  3.7%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of Correct Answers per Problem\")\n",
    "print(\"=\" * 50)\n",
    "correct_dist = df_llama['num_correct'].value_counts().sort_index()\n",
    "for num_correct, count in correct_dist.items():\n",
    "    pct = count / len(df_llama) * 100\n",
    "    print(f\"  {num_correct}/4 correct: {count:3d} problems ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problems where LLaMA got 0/4 correct (hardest)\n",
      "================================================================================\n",
      "Total: 240 problems\n",
      "\n",
      "By Subject:\n",
      "  geometry: 40\n",
      "  intermediate_algebra: 40\n",
      "  precalculus: 36\n",
      "  algebra: 34\n",
      "  counting_and_probability: 34\n",
      "  number_theory: 33\n",
      "  prealgebra: 23\n",
      "\n",
      "By Level:\n",
      "  Level 1: 45\n",
      "  Level 2: 39\n",
      "  Level 3: 49\n",
      "  Level 4: 49\n",
      "  Level 5: 58\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProblems where LLaMA got 0/4 correct (hardest)\")\n",
    "print(\"=\" * 80)\n",
    "zero_correct = df_llama[df_llama['num_correct'] == 0]\n",
    "print(f\"Total: {len(zero_correct)} problems\\n\")\n",
    "\n",
    "print(\"By Subject:\")\n",
    "for subject, count in zero_correct['subject'].value_counts().items():\n",
    "    print(f\"  {subject}: {count}\")\n",
    "\n",
    "print(\"\\nBy Level:\")\n",
    "for level, count in zero_correct['level'].value_counts().sort_index().items():\n",
    "    print(f\"  Level {level}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problems where LLaMA got 4/4 correct (easiest)\n",
      "================================================================================\n",
      "Total: 13 problems\n",
      "\n",
      "By Subject:\n",
      "  algebra: 4\n",
      "  counting_and_probability: 3\n",
      "  prealgebra: 3\n",
      "  number_theory: 2\n",
      "  geometry: 1\n",
      "\n",
      "By Level:\n",
      "  Level 1: 5\n",
      "  Level 2: 2\n",
      "  Level 3: 2\n",
      "  Level 4: 3\n",
      "  Level 5: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProblems where LLaMA got 4/4 correct (easiest)\")\n",
    "print(\"=\" * 80)\n",
    "all_correct = df_llama[df_llama['num_correct'] == 4]\n",
    "print(f\"Total: {len(all_correct)} problems\\n\")\n",
    "\n",
    "print(\"By Subject:\")\n",
    "for subject, count in all_correct['subject'].value_counts().items():\n",
    "    print(f\"  {subject}: {count}\")\n",
    "\n",
    "print(\"\\nBy Level:\")\n",
    "for level, count in all_correct['level'].value_counts().sort_index().items():\n",
    "    print(f\"  Level {level}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of 0/4 correct problems\n",
      "================================================================================\n",
      "\n",
      "idx=0 [algebra] Level 1\n",
      "GT: (-3+sqrt(3))/3,(-3-sqrt(3))/3\n",
      "LLaMA answers: -0.6666666667(2), -1Â±sqrt(6)/3(1), -1(1)\n",
      "\n",
      "idx=5 [algebra] Level 1\n",
      "GT: -3,4\n",
      "LLaMA answers: [-3,4](1), 4,-3(1), 4,x=-3(1)\n",
      "\n",
      "idx=9 [algebra] Level 1\n",
      "GT: n<0\n",
      "LLaMA answers: 1(1), -1,1,3,4,5,6,7,8(1), hence,theexpressionisnegativew(1)\n",
      "\n",
      "idx=10 [algebra] Level 2\n",
      "GT: 14\n",
      "LLaMA answers: cross-multiplying:$x*3=12(x-3)(1), 15(1), 4(1)\n",
      "\n",
      "idx=11 [algebra] Level 2\n",
      "GT: 2\n",
      "LLaMA answers: 1.74(1), \\frac{2(a+3)}{3(a+2(1), 1.8769230769(1)\n",
      "\n",
      "idx=12 [algebra] Level 2\n",
      "GT: (1,35),(2,23),(3,17),(5,11),(7,8),(8,7),(11,5),(17,3),(23,2)\n",
      "LLaMA answers: (1,1)(1), 1,35,2,23,3,17,5,11(1), weknowthat$a$and$b$arefactorso(1)\n",
      "\n",
      "idx=14 [algebra] Level 2\n",
      "GT: 2x^2-5x+10\n",
      "LLaMA answers: 2x^2-x(1), (2x+14)/(x(1), 4x(1)\n",
      "\n",
      "idx=15 [algebra] Level 2\n",
      "GT: (x^2-2y^3)(x^2+2y^3)(x^4-2x^2y^3+4y^6)(x^4+2x^2y^3+4y^6)\n",
      "LLaMA answers: 0(1), (x^6-64y^18)=(x^6-64(1), (x^6-4y^18)^2(1)\n",
      "\n",
      "idx=16 [algebra] Level 2\n",
      "GT: none\n",
      "LLaMA answers: 0(1), 1(1), (-1(1)\n",
      "\n",
      "idx=21 [algebra] Level 3\n",
      "GT: (11-4sqrt(151))/34+(44+sqrt(151))/34i,(11+4sqrt(151))/34+(44\n",
      "LLaMA answers: x^2+y^2=4(1), sqrt(7)+3)/2,-(sqrt(7)+3)/2.(1), (20-17*sqrt(43))/17+(-5-4*sqrt(1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample of 0/4 correct problems\")\n",
    "print(\"=\" * 80)\n",
    "for _, row in zero_correct.head(10).iterrows():\n",
    "    print(f\"\\nidx={row['idx']} [{row['subject']}] Level {row['level']}\")\n",
    "    print(f\"GT: {str(row['ground_truth'])[:60]}\")\n",
    "    answers = Counter(row['llama_answers'])\n",
    "    top = answers.most_common(3)\n",
    "    print(f\"LLaMA answers: {', '.join([f'{str(a)[:30]}({c})' for a, c in top])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Analysis\n",
      "============================================================\n",
      "Majority vote accuracy: 72/350 (20.6%)\n",
      "Pass@1 (single sample):  16.1%\n",
      "Pass@4 (any of 4):       31.4%\n"
     ]
    }
   ],
   "source": [
    "print(\"Majority Vote Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_majority_answer(answers):\n",
    "    counts = Counter(answers)\n",
    "    if not counts:\n",
    "        return None, 0\n",
    "    most_common = counts.most_common(1)[0]\n",
    "    return most_common[0], most_common[1]\n",
    "\n",
    "majority_correct = 0\n",
    "majority_total = len(df_llama)\n",
    "\n",
    "for _, row in df_llama.iterrows():\n",
    "    majority_ans, count = get_majority_answer(row['llama_answers'])\n",
    "    if answers_match(majority_ans, row['ground_truth']):\n",
    "        majority_correct += 1\n",
    "\n",
    "print(f\"Majority vote accuracy: {majority_correct}/{majority_total} ({majority_correct/majority_total:.1%})\")\n",
    "print(f\"Pass@1 (single sample):  {pass_at_k_unbiased(df_llama['correct_flags'].tolist(), 1):.1%}\")\n",
    "print(f\"Pass@4 (any of 4):       {pass_at_k_unbiased(df_llama['correct_flags'].tolist(), 4):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy by Subject\n",
      "==================================================\n",
      "algebra                  :  13/ 50 (26.0%)\n",
      "counting_and_probability :  12/ 50 (24.0%)\n",
      "geometry                 :   4/ 50 (8.0%)\n",
      "intermediate_algebra     :  10/ 50 (20.0%)\n",
      "number_theory            :   8/ 50 (16.0%)\n",
      "prealgebra               :  19/ 50 (38.0%)\n",
      "precalculus              :   6/ 50 (12.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Majority Vote Accuracy by Subject\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for subject in sorted(df_llama['subject'].unique()):\n",
    "    subset = df_llama[df_llama['subject'] == subject]\n",
    "    correct = 0\n",
    "    for _, row in subset.iterrows():\n",
    "        majority_ans, _ = get_majority_answer(row['llama_answers'])\n",
    "        if answers_match(majority_ans, row['ground_truth']):\n",
    "            correct += 1\n",
    "    print(f\"{subject:<25}: {correct:3d}/{len(subset):3d} ({correct/len(subset):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy by Level\n",
      "==================================================\n",
      "Level 1:  18/ 70 (25.7%)\n",
      "Level 2:  22/ 70 (31.4%)\n",
      "Level 3:  15/ 70 (21.4%)\n",
      "Level 4:  10/ 70 (14.3%)\n",
      "Level 5:   7/ 70 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Majority Vote Accuracy by Level\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for level in sorted(df_llama['level'].unique()):\n",
    "    subset = df_llama[df_llama['level'] == level]\n",
    "    correct = 0\n",
    "    for _, row in subset.iterrows():\n",
    "        majority_ans, _ = get_majority_answer(row['llama_answers'])\n",
    "        if answers_match(majority_ans, row['ground_truth']):\n",
    "            correct += 1\n",
    "    print(f\"Level {level}: {correct:3d}/{len(subset):3d} ({correct/len(subset):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_judge_debate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
